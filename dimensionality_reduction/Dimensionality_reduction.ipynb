{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Reload modules on change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.manifold import MDS, LocallyLinearEmbedding\n",
    "\n",
    "# Custom plotting\n",
    "from plotly_util import scatter_matrix, scatter_matrix_3d\n",
    "\n",
    "# Umbrella\n",
    "from umbrella import Umbrella\n",
    "\n",
    "# Dataset-specific\n",
    "from timecourse_util import timecourse_marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "![dimensionality reduction](img/Dimensionality-reduction-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Dimensionality reduction refers to a family of mathematical techniques that represent high-dimensional objects in lower dimensions.\n",
    "\n",
    "Dimensionality reduction is performed for two main reasons - to explore the relationships within the data and to reduce the number of components for further analysis.\n",
    "\n",
    "Say we have a dataset with $n=50$ features (columns). If we want to only use $n'=2$ of them to make a plot, we can start taking pairs of columns in our data, and plotting each one in term. In this case, we would need $\\frac{n(n-1)}{2}$ plots to show all the combinations. Each such plot is a projection of the data into 2-dimensional space, or simply its 'shadow'. \n",
    "\n",
    "Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stochastic Umbrellas\n",
    "\n",
    "To start with an example, let us consider a probabilistic model of an umbrella.\n",
    "\n",
    "![umbrella](img/bright-rainbow-umbrella.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umbrella = Umbrella(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "umbrella.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA - Principal component analysis\n",
    "\n",
    "_Find axes with maximum variance_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "PCA is the simplest technique for dimensionality reduction. One way to think about PCA is as a rotation and scaling of our data.\n",
    "\n",
    "PCA finds a new set of axes, along which the variance is maximized. It also scales the data along these axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA \n",
    "\n",
    "[Explained Visually - Eigenvectors](http://setosa.io/ev/eigenvectors-and-eigenvalues/)\n",
    "\n",
    "[Explained Visually -PCA](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "* Find the covariance matrix of the data\n",
    "* Find the eigenvectors of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umbrella_pca = PCA(n_components=3).fit_transform(umbrella.matrix)\n",
    "scatter_matrix(umbrella_pca, marker = umbrella.marker, \n",
    "               title=\"Principal component analysis\", \n",
    "               x_label=\"PC1\", y_label=\"PC2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scatter_matrix(umbrella_pca, dims=[1,2], marker = umbrella.marker, \n",
    "               title=\"Principal component analysis\", \n",
    "               x_label=\"PC2\", y_label=\"PC3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Let's get all the prinicpal components\n",
    "umbrella_pca = PCA(n_components=3).fit(umbrella.matrix)\n",
    "\n",
    "umbrella_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice above that `PC1` is close to $(1, 0, 0)$. This happens because the handle of the umbrella contains most variance, and becomes the first principal component. `PC2` and `PC3` are along the canopy ($z$ component is close to zero). Notice that they are orthogonal (perpendicular vectors). This orthogonality is one of the key properties of PCA - it does not reshape the original object (only rotates and scales it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check how much variance is caprtured\n",
    "umbrella_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check orthogonality\n",
    "print(np.dot(umbrella_pca.components_[:,0], umbrella_pca.components_[:,1]))\n",
    "print(np.dot(umbrella_pca.components_[:,1], umbrella_pca.components_[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA discussion\n",
    "\n",
    "## Pros\n",
    "\n",
    "* Very fast\n",
    "* Does not deform the data\n",
    "\n",
    "## Cons\n",
    "\n",
    "* Variance captured may be small\n",
    "* Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDS - Multidimensional scaling\n",
    "\n",
    "_Minimize stress of space embedding_\n",
    "\n",
    "$$Stress = \\sqrt{\\frac{\\sum(f(x)-d)^2}{\\sum d^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The idea behind MDS is to take the distances between points in the dataset and try to represent them in smaller number of dimensions. Since it is not possible to perfectly represent an object in lower dimensions, we need a measure that allows us to tell how badly we do.\n",
    "\n",
    "One such measure is _stress_. It expresses how much the object is deformed by projecting it.\n",
    "\n",
    "Above, $d$ is some distance between points, and $f(x)$ is the transformation of our data. So this formula is the sum of the distance differences between the two spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Euclidean distances\n",
    "\n",
    "$$d_{ij}=\\sqrt{\\sum_q(q_i-q_j)^2}$$\n",
    "\n",
    "\n",
    "$$Stress_D(x_1, x_2, ..., x_N) = \\sqrt{ \\sum_{i \\ne j = 1,...,N}(d_{ij} - \\|x_i - x_j\\|)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the equation above $d_{ij}$ is the distance between points $x_i$ and $x_j$ (in the high-dimensional space), and $\\|x_i-x_j\\|$ is the low-dimensional distance. \n",
    "\n",
    "Our task is to find an arrangement of points in low-dimensional space that minimize the stress function. In case of Euclidean distances, we can use linear algebra to find solutions efficiently. Usually, we can use standard optimization algorithms to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umbrella_mds = MDS(n_components=2).fit_transform(umbrella.matrix)\n",
    "scatter_matrix(umbrella_mds, umbrella.marker, \n",
    "               title=\"Multidimensional scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MDS discussion\n",
    "\n",
    "## Pros\n",
    "\n",
    "* Less assumptions about the data\n",
    "* Better for visualization\n",
    "\n",
    "## Cons\n",
    "\n",
    "* Slower\n",
    "* Harder to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ICA - Independent Component Analysis\n",
    "\n",
    "_Separate non-Gaussian components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ICA](img/ICA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Facial recognition\n",
    "\n",
    "[Face Recognition by Independent Component Analysis](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2898524/)\n",
    "\n",
    "Bartlett, Movellan, Sejnowski, 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA components\n",
    "\n",
    "![pca-faces](img/pca-faces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ICA components\n",
    "\n",
    "![pca-faces](img/ica-faces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There multiple implementations of the ICA algorithm. Here, we focus on FastICA, which is the de-facto standard.\n",
    "\n",
    "FastICA maximizes non-Gaussianity of the components, which is very close to independence. Intuitively, the join Gaussian distribution is completely symmetric, so one can not tease apart the components by any linear transformation.\n",
    "\n",
    "Essentially, FastICA maximizes the approximation 'negentropy' property. Negnetropy is the difference of information content between a variable and its Gaussian counterpart. The Gaussian equivalent is a Gaussian random variable that has the same covariance matrix as the original variable.\n",
    "\n",
    "Negentroy is hard to calculate directly, so it is found by kurtosis-based approximations. Kurtosis of a normally distributed variable is 0. Non-Gaussian variable have non-zero kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umbrella_ica = FastICA(n_components = 2).fit_transform(umbrella.matrix)\n",
    "scatter_matrix(umbrella_ica, umbrella.marker, title=\"Independent component analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ICA discussion\n",
    "\n",
    "## Pros\n",
    "\n",
    "* Detects descriptive features\n",
    "\n",
    "## Cons\n",
    "\n",
    "* Assumes additive interactions\n",
    "\n",
    "## Further reading\n",
    "\n",
    "* [FastICA Paper](https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLE - Locally Linear Embedding\n",
    "\n",
    "_Minimize stress of embedding neighborhoods_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LLE](img/LLE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "LLE reconstructs the high-dimensional space with patches of low dimension. We can think of this a cutting up small fragments of a sphere and fitting them together on a sheet of paper, while trying to keep the angles unchanged.\n",
    "\n",
    "As a result, we can \"unravel\" the different sections of the underlying geometry as separate parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Start with finding $k$ nearest neighbors\n",
    "\n",
    "2. Minimize the familiar embedding function:\n",
    "\n",
    "    $\\epsilon(W) = \\sum_i \\bigg\\rvert X_i - \\sum_j W_{ij} X_j \\bigg\\rvert ^2$\n",
    "\n",
    "    In this case, we add two important constraints:\n",
    "\n",
    "    * $W$ only has entries for the neighboors\n",
    "    * Rows of $W$ sum to $1$\n",
    "    \n",
    "\n",
    "3. While keeping $W$ fixed, minimize the embedding function in a lower dimension for _all_ the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umbrella_lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10).fit_transform(umbrella.matrix)\n",
    "scatter_matrix(umbrella_lle, umbrella.marker,\n",
    "               title=\"Locally linear embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LLE Discussion\n",
    "\n",
    "## Pros\n",
    "\n",
    "* Fast\n",
    "* Robust\n",
    "\n",
    "## Cons\n",
    "\n",
    "* Requires $k$ to be specified\n",
    "* Can perform poorly with large $k$ and small $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![paper](img/paper-front.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![time-course-rna-seq](img/trapnell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "du -h ../data/expression_matrix.csv\n",
    "head ../data/expression_matrix.csv | cut -d',' -f 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression = pd.read_csv(\"../data/expression_matrix.csv\", index_col=0)\n",
    "expression.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "expression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression_marker = timecourse_marker(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression_pca = PCA(n_components=2).fit_transform(expression)\n",
    "scatter_matrix(expression_pca, expression_marker, title=\"Cell expression profile PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression_mds = MDS(n_components=2).fit_transform(expression)\n",
    "scatter_matrix(expression_mds, expression_marker, title=\"Cell expression profile MDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression_ica = FastICA(n_components=2).fit_transform(expression)\n",
    "scatter_matrix(expression_ica, expression_marker, title=\"Cell expression profile ICA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expression_mlle = LocallyLinearEmbedding(n_neighbors=10, \n",
    "                                         n_components=2).fit_transform(expression)\n",
    "\n",
    "scatter_matrix(expression_mlle, expression_marker, title=\"Cell expression profile LLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_section_display": "block",
   "toc_threshold": 6,
   "toc_window_display": false
  },
  "toc_position": {
   "height": "650px",
   "left": "1400.91px",
   "right": "20px",
   "top": "124px",
   "width": "270px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
