{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Support Vector Machines\n",
    "\n",
    "_Learn by maximizing margin separation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why SVMs?\n",
    "\n",
    "* One of the better _off-the shelf_ algorithms\n",
    "\n",
    "* Optimize an intuitive notion of separation\n",
    "\n",
    "* Non-linear behaviour with linear runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SVM](images/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![svm-example](images/svm-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Margin Classifier\n",
    "\n",
    "Objective:\n",
    "\n",
    "$$max_{\\omega,b}\\gamma$$\n",
    "\n",
    "same as:\n",
    "\n",
    "$$min_{\\omega,b}\\frac{1}{2}\\|\\omega\\|^2$$\n",
    "\n",
    "while:\n",
    "\n",
    "$$y_i(\\omega x_i+b)\\ge 1, i=1,...,m$$\n",
    "\n",
    "Above, all points where $\\gamma=1$ are _closest_ to the margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Are we done?\n",
    "\n",
    "* We _can_ just leave it here\n",
    "* However, we notice that boundary only depends on the _closest_ points ($\\gamma=1$)\n",
    "    * This gives us an additional constraint\n",
    "    * Label active points with $\\alpha \\ne 0$\n",
    "    * Under **Karush-Kuhn-Tucker** (KKT) conditions, find the support vectors _and_ $\\omega$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Support Vectors\n",
    "\n",
    "![support-vectors](images/svm-support-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Margins for Support Vectors\n",
    "\n",
    "$$\\omega = \\sum_{i=0}^m \\alpha_i y_i x_i$$\n",
    "\n",
    "While\n",
    "\n",
    "$$\\sum_{i=0}^m \\alpha_i y_i = 0$$\n",
    "\n",
    "Now, if we have $\\alpha$, we can find $\\omega$ easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intermezzo:\n",
    "\n",
    "## Separability in higher dimensions\n",
    "\n",
    "[Eric Kim's Kernels Page](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)\n",
    "\n",
    "![hd-sep](images/data_2d_to_3d_hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernel Trick\n",
    "\n",
    "$$\\begin{align}\n",
    "\\omega x + b &= \\left( \\sum_{i=0}^m \\alpha_i y_i x_i \\right) x + b \\\\\n",
    "&= \\sum_{i=0}^m \\alpha_i y_i \\langle x_i,x\\rangle + b\n",
    "\\end{align}$$\n",
    "\n",
    "Remember that most $\\alpha$s are zero.\n",
    "\n",
    "Dot products of functions are often simpler than functions of dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernels\n",
    "\n",
    "_Kernels are small functions_\n",
    "\n",
    "For SVMs, a _kernel_ is defined as inner product of feature transformations $\\phi$:\n",
    "\n",
    "$$K(x, z) = \\phi(x)^T \\phi(x)$$\n",
    "\n",
    "The kernels allow SVM to learn from high-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernel Example\n",
    "\n",
    "In general:\n",
    "\n",
    "$$K(x, z) = \\phi(x)^T \\phi(x)$$\n",
    "\n",
    "Let's say we want to fit a polynomial transformation:\n",
    "\n",
    "$$ K(x,z) = (x^T z)^2 $$\n",
    "\n",
    "We can calculate $\\phi$ directly, but that is quite hard. For $n=2$:\n",
    "\n",
    "$$K(x,z)=\\left(\\begin{bmatrix}x_1 x_1 \\\\ x_1 x_2 \\\\ x_2 x_1 \\\\ x_2 x_2 \\end{bmatrix} \n",
    "\\begin{bmatrix} z_1 z_1 & z_1 z_2 & z_2 z_1 & z_2 z_2\\end{bmatrix} \\right) ^{2}$$\n",
    "\n",
    "However, we can simplify this:\n",
    "\n",
    "$$\\begin{align} K(x,z) &=\\left( \\sum_{i=1}^n x_i z_i \\right) \\left( \\sum_{j=1}^n x_i z_i \\right) \\\\\n",
    "&=\\sum_{i,j=1}^n (x_i x_j) (z_i z_j)\n",
    "\\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Common Kernels\n",
    "\n",
    "* Linear \n",
    "    * $\\langle x, z \\rangle$\n",
    "* Polynomial \n",
    "    * $(\\gamma \\langle x, z \\rangle + r)^d$\n",
    "* Gaussian Radial Basis (RBF) \n",
    "    * $e^{-\\gamma(\\| x - z \\|^2)}$\n",
    "* Sigmoid\n",
    "    * $tanh(\\gamma\\langle x, z \\rangle+r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional materials\n",
    "\n",
    "Andrew Ng's lectures for CS229, Stanford"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
