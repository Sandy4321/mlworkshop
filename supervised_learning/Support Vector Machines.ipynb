{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM - Support Vector Machines\n",
    "\n",
    "_Learn by maximizing margin separation_\n",
    "\n",
    "![SVM](images/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "SVM is a very robust family of machine learning algorithms.\n",
    "\n",
    "Here, we will consider a supervised classification problem with two classes in some detail.\n",
    "\n",
    "Intuitively, SVMs try to find a boundary separating the different classes of observations. This boundary is always linear. The larger the gap separating the different classes, the more confidence we have in the prediction. \n",
    "\n",
    "We call this separation gap a margin, and our objective is to chose the largest margin. In the figure above, the margin is the green line, and our objective is to maximize the distance from the margin to the closest data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![svm-example](images/svm-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Margin Classifier\n",
    "\n",
    "$$max_{\\omega,b}\\gamma$$\n",
    "\n",
    "same as:\n",
    "\n",
    "$$min_{\\omega,b}\\frac{1}{2}\\|\\omega\\|^2$$\n",
    "\n",
    "while:\n",
    "\n",
    "$$y_i(\\omega x_i+b)\\ge 1, i=1,...,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to find the values of $\\omega$ and $b$ that maximize the margin $\\gamma$. This turns out to be equivalent to minimizing the square size (norm) of the vector $\\omega$, as above.\n",
    "\n",
    "In addition, we want to constrain the sizes of the margin to be larger or equal to $1$. In this case, the points with $\\omega=1$ will be exactly the closest to the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vectors\n",
    "\n",
    "![support-vectors](images/svm-support-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can leave it at that, and just try to optimize the formulation above. However, we can notice that the margin only depends on the points closest to the separation boundary. These are the points where $\\omega=1$. This adds an extra step to the problem setup - but turns out to simplify the problem. We can optimize with additional constrains, including that only some points are _active_. These our our support vectors.\n",
    "\n",
    "By using Lagrange multipliers, a general approach for constrained optimization, under **Karush-Kuhn-Tucker** conditions, we can find optimal support vectors for a particular margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Margins for Support Vectors\n",
    "\n",
    "$$\\omega = \\sum_{i=0}^m \\alpha_i y_i x_i$$\n",
    "\n",
    "While\n",
    "\n",
    "$$\\sum_{i=0}^m \\alpha_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given the values $\\alpha$, the non-zero Lagrange multipliers of the support vectors, we can now find the $\\omega$ quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Separability in higher dimensions\n",
    "\n",
    "[Eric Kim's Kernels Page](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)\n",
    "\n",
    "![hd-sep](images/data_2d_to_3d_hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernel Trick\n",
    "\n",
    "$$\\begin{align}\n",
    "\\omega x + b &= \\left( \\sum_{i=0}^m \\alpha_i y_i x_i \\right) x + b \\\\\n",
    "&= \\sum_{i=0}^m \\alpha_i y_i \\langle x_i,x\\rangle + b\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, if we want to find the margin for a new point, we use the margin function $\\omega x + b$. We can express this using the formulation with $\\alpha$ from before. Most of $\\alpha$s are zero, so this inner product is easier to calculate. In addition, it gives us extra power, since it is often possible to find the inner product of two transformed vectors without finding the transformed vectors themselves. This gives SVMs much of their power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernels\n",
    "\n",
    "_Kernels are small functions_\n",
    "\n",
    "For SVMs, a _kernel_ is defined as inner product of feature transformations $\\phi$:\n",
    "\n",
    "$$K(x, z) = \\phi(x)^T \\phi(x)$$\n",
    "\n",
    "The kernels allow SVM to learn from high-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kernel Example\n",
    "\n",
    "Let's say we want to fit a polynomial transformation:\n",
    "\n",
    "$$ K(x,z) = (x^T z)^2 $$\n",
    "\n",
    "However, we can simplify this:\n",
    "\n",
    "$$\\begin{align} K(x,z) &=\\left( \\sum_{i=1}^n x_i z_i \\right) \\left( \\sum_{j=1}^n x_i z_i \\right) \\\\\n",
    "&=\\sum_{i,j=1}^n (x_i x_j) (z_i z_j)\n",
    "\\end{align} $$\n",
    "\n",
    "On the other side, calculating $\\phi$ directly is hard. For $n=2$:\n",
    "\n",
    "$$\\phi(x)=\\begin{bmatrix}x_1 x_1 \\\\ x_1 x_2 \\\\ x_2 x_1 \\\\ x_2 x_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Common Kernels\n",
    "\n",
    "* Linear \n",
    "    * $\\langle x, z \\rangle$\n",
    "* Polynomial \n",
    "    * $(\\gamma \\langle x, z \\rangle + r)^d$\n",
    "* Gaussian Radial Basis (RBF) \n",
    "    * $e^{-\\gamma(\\| x - z \\|^2)}$\n",
    "* Sigmoid\n",
    "    * $tanh(\\gamma\\langle x, z \\rangle+r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional materials\n",
    "\n",
    "Andrew Ng's lectures for CS229, Stanford"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
