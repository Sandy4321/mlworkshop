{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Adjusted mutual information and silhouette\n",
    "* Choosing number of clusters via elbow method\n",
    "* Doing dimensionality reduction to avoid curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering as unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem: clustering gene expression data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trapnell (2014) performed single-cell RNA-seq on differentiating skeletal myoblasts at 0, 24, 48, and 72 hours. The original data are available via [GEO database accession number GSE52529](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE52nnn/GSE52529/suppl/GSE52529_fpkm matrix.txt.gz). We have at hand a cleaned-up version from [Sincell](https://www.bioconductor.org/packages/3.3/bioc/vignettes/sincell/inst/doc/sincell-vignette.pdf), in which only the expression levels for 575 differentially-expressed genes have been retained across 271 cells drawn from the various timepoints.\n",
    "\n",
    "Our goal is to cluster the 271 cells based on gene expression. *Clustering* is simply a means of grouping related items in your data together, allowing you to gain insight into overarching trends. In this case, we expect cells at each timepoint to have relatively similar expression levels across different genes, meaning that we hope cells from the same timepoints will cluster together. In this regard, knowing what timepoint each cell comes from provides us with a \"ground truth\" means of evaluating our clustering&mdash;if we see cells from the same timepoint present in the same cluster, it means our clustering is working as intended. This will give us confidence in applying our clustering methods to other single-cell datasets, where we hope to discover underlying structure based on factors other than time since cell division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How k-means clustering works\n",
    "\n",
    "K-means is an extremely simple clustering algorithm that is nonetheless quite effective.\n",
    "\n",
    "1. Decide how many clusters $k$ you wish to use.\n",
    "2. Place $k$ cluster centres (\"centroids\") at random locations in the space in which your data lives.\n",
    "3. Assign each data point to the cluster corresponding to its closest centroid.\n",
    "4. Repeat:\n",
    "    1. For each cluster, move its centroid to the centre (i.e., mean) of all points belonging to the cluster.\n",
    "    2. Assign each data point to its closest centroid.\n",
    "    3. If no points changed their cluster assignments, exit. Otherwise, return to step 4.\n",
    "\n",
    "The following example illustrates the algorithm. Cluster centroids are circles, while data points are squares. Here, we have arbitrarily chosen to create $k=3$ clusters.\n",
    "\n",
    "| Step | Illustration |\n",
    "| ---- | ------------ |\n",
    "| Step 1: Place $k=3$ centroids randomly. Here, we have arbitrariliy chosen $k=3$&mdash;we could have easily used a different value of $k$. | ![K-means step 1](images/kmeans_example_step1.svg) |\n",
    "| Step 2: Assign each data point to its closest centroid. | ![K-means step 2](images/kmeans_example_step2.svg) |\n",
    "| Step 3: Move each centroid to the centre of all data points belonging to its cluster. | ![K-means step 3](images/kmeans_example_step3.svg) |\n",
    "| Step 4:Reassign each data point to the cluster with the closest centroid. | ![K-means step 4](images/kmeans_example_step4.svg) \n",
    "\n",
    "We repeat these steps until convergence&mdash;that is, when no points change cluster assignments. Note that the precise solution you obtain will depend on the random locations you initially placed your centroids. Rerunning the algorithm with different starting points for the centroids may change the cluster assignments of some points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fit a mixture of Gaussians using expectation maximization\n",
    "\n",
    "As an alternative to k-means, we can fit a *mixture of Gaussians* using *expectation maximization*. Let's first discuss what a mixture of Gaussians is. Imagine you have a bunch of points that you know were sampled from different Gaussian probability distributions.\n",
    "\n",
    "![MOG truth](images/mog_truth.png)\n",
    "\n",
    "Here, you see points drawn from three different Gaussians. Each Gaussian has parameters describing its mean and variance, which determine the position and shape of the distribution. As there is more probability mass under the \"centre\" of the Guassians than under the \"tails\", we see more points sampled under the centre of each distribution. But, of course, if you were to encounter this data in the real world, you wouldn't know the parameters of the Gaussians, so you would have no idea what the distributions look like.\n",
    "\n",
    "![MOG truth](images/mog_no_pdfs.png)\n",
    "\n",
    "In fact, you don't even know which Gaussian each point was sampled from.\n",
    "\n",
    "![MOG truth](images/mog_no_labels.png)\n",
    "\n",
    "However, to reduce the complexity of the problem, let's assume for the moment that you do know that the data was drawn from Gaussian distributions, however, and that there were originally three Gaussians. Given this knowledge, how do you determine the parameters for each Gaussian, and consequently what distribution each point belongs to? The answer is expectation maximization.\n",
    "\n",
    "Expectation maximization is an iterative algorithm that lets you establish both the parameters of the distributions from which your data was generated, as well as the most likely assignments of points to distributions. As such, in our example, we can think of each Gaussian corresponding to a cluster. To determine the cluster parameters and assignments, we use the following algorithm:\n",
    "\n",
    "1. E (expectation) step: Each cluster has associated with it parameters describing its properties. For our Gaussian example, this corresponds to a mean and variance for each Gaussian describing the position and shape of the distribution. For each data point, determine the *likelihood* that it was generated by each cluster.\n",
    "\n",
    "2. M (maximization) step: For each data point and each cluster, update the parameters of the cluster based on the points assigned to it.\n",
    "\n",
    "3. If the cluster assignments and parameters hardly changed (i.e., converged) in the E and M steps, exit. Otherwise return to step 1.\n",
    "\n",
    "![MOG all steps](images/mog_all_steps.png)\n",
    "\n",
    "Here we observe this process in action.\n",
    "\n",
    "* In step 1, we initialize three Gaussians with random means and variances.\n",
    "* In step 2, we have performed our first E and M steps. Each point is associated with its most likely Gaussian (E step); each Gaussian is adjusted to better fits its associated points (M step).\n",
    "* In step 3, we perform another run of the E and M steps.\n",
    "* In step 4, the algorithm has converged and so we terminate. The answer we arrive at precisely matches the ground truth.\n",
    "\n",
    "What is not captured by the image is that, unlike k-means, expectation maximization generates *soft* rather than *hard* assignments of points to clusters. K-means' assignments are \"hard\" because, at each step, every point is assigned with certainty to one and only one cluster. Expectation maximization, conversely, generates \"soft\" assignments, as every point is assigned with some probability to every cluster. This has two consequences. Firstly, the higher the probability of a point's assignment to a given cluster, the more \"weight\" that point will have in determining that cluster's parameters. For example, in step 2 of our EM example, the points directly under the centre of the red Gaussian will have a large influence in determining the updated mean of the distribution for step 3. But even the green points on the far right of the axis will have a small influence on the new mean of the red Gaussian, in accordance with their infinitesimal probability of having been generated by the red Gaussian. Once the algorithm converges, we can get hard assignments of points to clusters by simply taking the most probable cluster for each point.\n",
    "\n",
    "Relative to k-means, expectation maximization is a more general algorithm that can be used for purposes other than clustering. In fact, EM can be applied to a wide class of problems in which we confront *latent* (hidden) variables that affect our inference. In this example, the latent variables (or hidden knowledge) are which distribution generated each point. But you can also apply EM to diverse other problems. Imagine you were flipping two coins, each with some unknown bias towards or tails. Given the results of 100 coin flips (heads, tails, heads, heads, ...), in which one coin or the other was selected randomly at each step, you want to know which coin was flipped at each step, and what bias each coin has. This problem appears quite difficult, as you lack knowledge not only of the bias parameter associated with each coin, but also the identity of the coin used for each flip. Expectation maximization gives you a principled way of determining the most likely solution to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.40668197  1.68653758]\n",
      " [ 1.56975907  1.59487466]\n",
      " [-4.93425895  4.56220774]\n",
      " [ 2.36019663  1.14894076]\n",
      " [-3.84745114  2.14805512]\n",
      " [ 3.5661392   1.42438541]\n",
      " [-0.69261162 -3.94876848]\n",
      " [-2.37621142 -5.3188365 ]\n",
      " [-3.87588566  1.93759412]\n",
      " [-4.39707614  3.84360606]\n",
      " [-1.90152152 -4.80069395]\n",
      " [ 2.36127428  1.95124545]\n",
      " [-5.57689135  2.91115443]\n",
      " [-6.03912866  4.2594258 ]\n",
      " [-1.7897726  -4.24561122]\n",
      " [ 3.205313    2.70060585]\n",
      " [ 3.80073222  2.67239015]\n",
      " [-2.40567392 -4.6089913 ]\n",
      " [ 2.19428562  1.80213024]\n",
      " [-4.43089198  3.9666052 ]\n",
      " [ 3.98833067  2.01947583]\n",
      " [-1.49338779 -4.02730121]\n",
      " [-3.17080414 -5.39216147]\n",
      " [-3.42879913  3.42666611]\n",
      " [-6.26750821  2.75198668]\n",
      " [ 3.55078108  3.26553442]\n",
      " [-2.70362091 -5.97072686]\n",
      " [ 2.65859808 -0.02724194]\n",
      " [-3.8226584  -3.78325589]\n",
      " [-3.284733   -4.03736813]] [ 0.  2.  0.  2.  0.  2.  1.  1.  0.  0.  1.  2.  0.  0.  1.  2.  2.  1.\n",
      "  2.  0.  2.  1.  1.  0.  0.  2.  1.  2.  1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"7eb863eb-107a-4c2f-a867-1ab653f1cd30\" style=\"height: 525; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7eb863eb-107a-4c2f-a867-1ab653f1cd30\", [{\"text\": [0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0], \"mode\": \"markers\", \"marker\": {\"symbol\": [\"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\"], \"colorscale\": \"RdYlBu\", \"color\": [0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0]}, \"type\": \"scatter\", \"x\": [-3.4066819662226084, 1.5697590738337182, -4.93425895174691, 2.3601966341101117, -3.8474511433586804, 3.566139199060971, -0.6926116233707016, -2.3762114204856295, -3.87588566450502, -4.3970761350057685, -1.901521520368264, 2.3612742763613106, -5.576891350357848, -6.039128660921122, -1.7897725994845355, 3.2053130006072283, 3.8007322194059645, -2.405673920036004, 2.194285617786257, -4.430891976477508, 3.9883306670530656, -1.4933877896529126, -3.1708041361376624, -3.428799128035789, -6.267508209367966, 3.550781083045957, -2.7036209082008353, 2.658598079635556, -3.8226584041200526, -3.2847330033216253], \"y\": [1.6865375764534747, 1.5948746623048844, 4.562207743893495, 1.1489407588074667, 2.148055119471297, 1.4243854092180261, -3.948768482979142, -5.318836500080753, 1.9375941176334208, 3.8436060641609595, -4.800693947618063, 1.9512454512873931, 2.9111544294817406, 4.2594258012049036, -4.245611223909553, 2.7006058476406594, 2.672390154918677, -4.608991295592001, 1.8021302436293534, 3.966605203839573, 2.0194758308074587, -4.027301207288345, -5.392161466413105, 3.4266661129333227, 2.7519866762218794, 3.2655344180574546, -5.970726859169199, -0.027241944377222715, -3.783255893719474, -4.037368134494117]}], {\"xaxis\": {\"range\": [0, 1], \"title\": \"pants\"}, \"yaxis\": {\"range\": [0, 1], \"title\": \"pants\"}, \"title\": \"pants\", \"hovermode\": \"closest\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cluster import run_simulated\n",
    "run_simulated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to cluster our gene expression data\n",
    "\n",
    "We have a 271x575 matrix containing gene expression levels. Each row corresponds to a cell, while each column corresponds to the expression of a gene in $\\log\\text{FPKM}$ units. We will perform dimensionality reduction via PCA on our data before clustering, for reasons that we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: (271, 575)\n",
      "Explained variance ratio: [ 0.22354609  0.09390967]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cluster import get_timepoints, plot\n",
    "import sklearn.decomposition\n",
    "\n",
    "def load_exprmat(exprmat_fn):\n",
    "  rows = []\n",
    "  rownames = []\n",
    "\n",
    "  with open(exprmat_fn) as exprmat:\n",
    "    colnames = next(exprmat).strip().split(',')\n",
    "    for row in exprmat:\n",
    "      fields = row.split(',')\n",
    "      rownames.append(fields[0])\n",
    "      rows.append([float(f) for f in fields[1:]])\n",
    "\n",
    "  data = np.array(rows)\n",
    "  return (data, colnames, rownames)\n",
    "\n",
    "def reduce_dimensionality(exprmat, n_components):\n",
    "  pca = sklearn.decomposition.PCA(n_components=n_components)\n",
    "  projected = pca.fit(exprmat).transform(exprmat)\n",
    "  print('Explained variance ratio: %s' % pca.explained_variance_ratio_)\n",
    "  return projected\n",
    "\n",
    "exprmat, genes, samples = load_exprmat('../data/expression_matrix.csv')\n",
    "print('Matrix size:', exprmat.shape)\n",
    "timepoints = get_timepoints(samples)\n",
    "projected = reduce_dimensionality(exprmat, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cluster the cells, we will use the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering our gene expression data using k-means\n",
    "\n",
    "Running k-means is simple. Note that we must tell the algorithm how many clusters we wish to find. In this case, as we hope to see our clusters echo the timepoint assignment of each cell, and because we know there are four timepoints present in our data, we fix the number of clusters at four. Choosing the best cluster number when you lack knowledge *a priori* of the \"right\" answer is difficult, as we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "preds = sklearn.cluster.KMeans(n_clusters=4).fit_predict(exprmat)\n",
    "plot_expression('kmeans', projected, preds, timepoints, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering via Gaussian mixture models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.mixture\n",
    "preds = sklearn.mixture.GMM(n_components=4).fit_predict(exprmat)\n",
    "plot_expression('GMM', projected, preds, timepoints, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dirichlet process Gaussian mixture models to select the best number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = sklearn.mixture.DPGMM(n_components=100, alpha=0.3).fit_predict(exprmat)\n",
    "plot_expression('DPGMM', projected, preds, timepoints, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating clustering success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best number of clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
